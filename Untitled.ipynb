{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "partial-karen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FEAST_CORE_CONFIG': './core/core.yml',\n",
      " 'FEAST_HISTORICAL_FEATURE_OUTPUT_FORMAT': 'parquet',\n",
      " 'FEAST_HISTORICAL_FEATURE_OUTPUT_LOCATION': 'file:///tmp/historical_feature_output',\n",
      " 'FEAST_ONLINE_SERVING_CONFIG': './serving/online-serving.yml',\n",
      " 'FEAST_SPARK_HOME': '/opt/spark',\n",
      " 'FEAST_SPARK_LAUNCHER': 'standalone',\n",
      " 'FEAST_SPARK_STAGING_LOCATION': 'file:///tmp/staging',\n",
      " 'FEAST_SPARK_STANDALONE_MASTER': 'local',\n",
      " 'FEAST_VERSION': 'develop'}\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "from pprint import pprint\n",
    "pprint({key: value for key, value in os.environ.items() if key.startswith(\"FEAST_\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-merit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geographic-financing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from feast import Client as FeastClient, Entity, Feature, ValueType, FeatureTable\n",
    "from feast.data_source import FileSource\n",
    "from feast.data_format import ParquetFormat\n",
    "from feast.constants import ConfigOptions as opt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from redisai import Client as RedisAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hired-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvirdukhan/Code/feast/feast_env/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PONG'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feast import Client as FeastClient\n",
    "from feast.constants import ConfigOptions as opt\n",
    "feast_client = FeastClient()\n",
    "# jar = {key: value for key, value in os.environ.items() if key.startswith(\"INGESTION_\")}[\"INGESTION_JAR_PATH\"]\n",
    "# print(jar)\n",
    "# feast_client._config.set(opt.SPARK_INGESTION_JAR, jar)\n",
    "feast_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-egyptian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redisai_client = RedisAIClient()\n",
    "redisai_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec:\n",
      "  name: poc_table\n",
      "  entities:\n",
      "  - row_id\n",
      "  features:\n",
      "  - name: a\n",
      "    valueType: INT32\n",
      "  - name: b\n",
      "    valueType: INT32\n",
      "  batchSource:\n",
      "    type: BATCH_FILE\n",
      "    eventTimestampColumn: datetime\n",
      "    createdTimestampColumn: created\n",
      "    fileOptions:\n",
      "      fileFormat:\n",
      "        parquetFormat: {}\n",
      "      fileUrl: file:///home/dvirdukhan/Code/feast/poc_table.parquet\n",
      "meta:\n",
      "  createdTimestamp: '2021-03-22T10:08:10Z'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "file_url = \"file://\" + os.path.join(cwd, \"poc_table.parquet\")\n",
    "row_id = Entity(name=\"row_id\", description=\"POC row identifier\", value_type=ValueType.INT64)\n",
    "a = Feature(\"a\", ValueType.INT32)\n",
    "b = Feature(\"b\", ValueType.INT32)\n",
    "poc_table = FeatureTable(name = \"poc_table\", entities=[\"row_id\"] ,features=[a, b], batch_source=FileSource(\n",
    "        event_timestamp_column=\"datetime\",\n",
    "        created_timestamp_column=\"created\",\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=file_url\n",
    "    ))\n",
    "feast_client.apply(row_id)\n",
    "feast_client.apply(poc_table)\n",
    "print(feast_client.get_feature_table(\"poc_table\").to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civil-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temporary file(s)...\n",
      "Data has been successfully ingested into FeatureTable batch source.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "row_id               int64\n",
       "a                    int32\n",
       "b                    int32\n",
       "created     datetime64[ns]\n",
       "datetime    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"row_id\", \"a\", \"b\", \"created\", \"datetime\"])\n",
    "df['row_id'] = np.array([0,1]).astype(np.int64)\n",
    "df['a'] = np.array([2,3]).astype(np.int32)\n",
    "df['b'] = np.array([2,3]).astype(np.int32)\n",
    "df['created'] = pd.to_datetime(datetime.now())\n",
    "df['datetime'] = pd.to_datetime(\n",
    "            np.random.randint(\n",
    "                datetime(2020, 10, 10).timestamp(),\n",
    "                datetime(2020, 10, 20).timestamp(),\n",
    "                size=2),\n",
    "        unit=\"s\"\n",
    "    )\n",
    "feast_client.ingest(poc_table, df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deadly-hospital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/spark/bin/spark-submit', '--master', 'local', '--name', 'BatchIngestion-poc_table-2020-10-10-2020-10-20', '--class', 'feast.ingestion.IngestionJob', '--conf', 'spark.ui.port=48709', '--conf', 'spark.executor.extraJavaOptions=-Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Duser.timezone=GMT', '--conf', 'spark.driver.extraJavaOptions=-Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Duser.timezone=GMT', '--conf', 'spark.sql.session.timeZone=UTC', '--packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0', '--jars', 'https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar,https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar,https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar', '--conf', 'spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem', '--conf', 'spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem', 'https://storage.googleapis.com/feast-jobs/spark/ingestion/feast-ingestion-spark-develop.jar', '--feature-table', '{\"features\": [{\"name\": \"a\", \"type\": \"INT32\"}, {\"name\": \"b\", \"type\": \"INT32\"}], \"project\": \"default\", \"name\": \"poc_table\", \"entities\": [{\"name\": \"row_id\", \"type\": \"INT64\"}], \"max_age\": null, \"labels\": {}}', '--source', '{\"file\": {\"field_mapping\": {}, \"event_timestamp_column\": \"datetime\", \"created_timestamp_column\": \"created\", \"date_partition_column\": \"\", \"path\": \"file:///home/dvirdukhan/Code/feast/poc_table.parquet\", \"format\": {\"json_class\": \"ParquetFormat\"}}}', '--redis', '{\"host\": \"localhost\", \"port\": 6379, \"ssl\": false}', '--mode', 'offline', '--start', '2020-10-10T00:00:00', '--end', '2020-10-20T00:00:00']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = feast_client.start_offline_to_online_ingestion(\n",
    "    poc_table,\n",
    "    datetime(2020, 10, 10),\n",
    "    datetime(2020, 10, 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-romania",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reduced-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/22 22:49:05 WARN Utils: Your hostname, LAPTOP-VQ3ERA4U resolves to a loopback address: 127.0.1.1; using 172.20.167.155 instead (on interface eth0)\n",
      "\n",
      "21/03/22 22:49:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "\n",
      "Ivy Default Cache set to: /home/dvirdukhan/.ivy2/cache\n",
      "\n",
      "The jars for the packages stored in: /home/dvirdukhan/.ivy2/jars\n",
      "\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      "\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-efd685a7-30ba-4e6b-afbb-3c8a39bee1d4;1.0\n",
      "\n",
      "\tconfs: [default]\n",
      "\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.18.0 in central\n",
      "\n",
      ":: resolution report :: resolve 135ms :: artifacts dl 2ms\n",
      "\n",
      "\t:: modules in use:\n",
      "\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.18.0 from central in [default]\n",
      "\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-efd685a7-30ba-4e6b-afbb-3c8a39bee1d4\n",
      "\n",
      "\tconfs: [default]\n",
      "\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "\n",
      "21/03/22 22:49:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Running Spark version 3.0.1\n",
      "\n",
      "21/03/22 22:49:28 INFO ResourceUtils: ==============================================================\n",
      "\n",
      "21/03/22 22:49:28 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "\n",
      "\n",
      "21/03/22 22:49:28 INFO ResourceUtils: ==============================================================\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Submitted application: BatchIngestion-poc_table-2020-10-10-2020-10-20\n",
      "\n",
      "21/03/22 22:49:28 INFO SecurityManager: Changing view acls to: dvirdukhan\n",
      "\n",
      "21/03/22 22:49:28 INFO SecurityManager: Changing modify acls to: dvirdukhan\n",
      "\n",
      "21/03/22 22:49:28 INFO SecurityManager: Changing view acls groups to: \n",
      "\n",
      "21/03/22 22:49:28 INFO SecurityManager: Changing modify acls groups to: \n",
      "\n",
      "21/03/22 22:49:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dvirdukhan); groups with view permissions: Set(); users  with modify permissions: Set(dvirdukhan); groups with modify permissions: Set()\n",
      "\n",
      "21/03/22 22:49:28 INFO Utils: Successfully started service 'sparkDriver' on port 46291.\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkEnv: Registering MapOutputTracker\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkEnv: Registering BlockManagerMaster\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "\n",
      "21/03/22 22:49:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-daba0581-3768-46f0-96d3-f23039c0b729\n",
      "\n",
      "21/03/22 22:49:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "\n",
      "21/03/22 22:49:28 INFO Utils: Successfully started service 'SparkUI' on port 48709.\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.20.167.155:48709\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Added JAR https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar at https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Added JAR https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar at https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Added JAR https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar at https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Added JAR file:///home/dvirdukhan/.ivy2/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.18.0.jar at spark://172.20.167.155:46291/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.18.0.jar with timestamp 1616453368824\n",
      "\n",
      "21/03/22 22:49:28 INFO SparkContext: Added JAR https://storage.googleapis.com/feast-jobs/spark/ingestion/feast-ingestion-spark-develop.jar at https://storage.googleapis.com/feast-jobs/spark/ingestion/feast-ingestion-spark-develop.jar with timestamp 1616453368824\n",
      "\n",
      "21/03/22 22:49:28 INFO Executor: Starting executor ID driver on host 172.20.167.155\n",
      "\n",
      "21/03/22 22:49:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32879.\n",
      "\n",
      "21/03/22 22:49:28 INFO NettyBlockTransferService: Server created on 172.20.167.155:32879\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.20.167.155, 32879, None)\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.167.155:32879 with 434.4 MiB RAM, BlockManagerId(driver, 172.20.167.155, 32879, None)\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.20.167.155, 32879, None)\n",
      "\n",
      "21/03/22 22:49:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.20.167.155, 32879, None)\n",
      "\n",
      "21/03/22 22:49:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dvirdukhan/Code/feast/spark-warehouse').\n",
      "\n",
      "21/03/22 22:49:29 INFO SharedState: Warehouse path is 'file:/home/dvirdukhan/Code/feast/spark-warehouse'.\n",
      "\n",
      "21/03/22 22:49:29 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.\n",
      "\n",
      "21/03/22 22:49:29 INFO SparkContext: Starting job: parquet at FileReader.scala:34\n",
      "\n",
      "21/03/22 22:49:29 INFO DAGScheduler: Got job 0 (parquet at FileReader.scala:34) with 1 output partitions\n",
      "\n",
      "21/03/22 22:49:29 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at FileReader.scala:34)\n",
      "\n",
      "21/03/22 22:49:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "\n",
      "21/03/22 22:49:29 INFO DAGScheduler: Missing parents: List()\n",
      "\n",
      "21/03/22 22:49:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at FileReader.scala:34), which has no missing parents\n",
      "\n",
      "21/03/22 22:49:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 74.8 KiB, free 434.3 MiB)\n",
      "\n",
      "21/03/22 22:49:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.3 MiB)\n",
      "\n",
      "21/03/22 22:49:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.167.155:32879 (size: 27.2 KiB, free: 434.4 MiB)\n",
      "\n",
      "21/03/22 22:49:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223\n",
      "\n",
      "21/03/22 22:49:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at FileReader.scala:34) (first 15 tasks are for partitions Vector(0))\n",
      "\n",
      "21/03/22 22:49:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "\n",
      "21/03/22 22:49:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.20.167.155, executor driver, partition 0, PROCESS_LOCAL, 7517 bytes)\n",
      "\n",
      "21/03/22 22:49:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "\n",
      "21/03/22 22:49:30 INFO Executor: Fetching https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:30 INFO Utils: Fetching https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar to /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/fetchFileTemp6150279755041665957.tmp\n",
      "\n",
      "21/03/22 22:49:34 INFO Executor: Adding file:/tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/gcs-connector-hadoop2-latest.jar to class loader\n",
      "\n",
      "21/03/22 22:49:34 INFO Executor: Fetching https://storage.googleapis.com/feast-jobs/spark/ingestion/feast-ingestion-spark-develop.jar with timestamp 1616453368824\n",
      "\n",
      "21/03/22 22:49:35 INFO Utils: Fetching https://storage.googleapis.com/feast-jobs/spark/ingestion/feast-ingestion-spark-develop.jar to /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/fetchFileTemp16393773937270974478.tmp\n",
      "\n",
      "21/03/22 22:49:45 INFO Executor: Adding file:/tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/feast-ingestion-spark-develop.jar to class loader\n",
      "\n",
      "21/03/22 22:49:45 INFO Executor: Fetching https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:45 INFO Utils: Fetching https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar to /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/fetchFileTemp18423679409486655776.tmp\n",
      "\n",
      "21/03/22 22:49:48 INFO Executor: Adding file:/tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/aws-java-sdk-1.7.4.jar to class loader\n",
      "\n",
      "21/03/22 22:49:48 INFO Executor: Fetching spark://172.20.167.155:46291/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.18.0.jar with timestamp 1616453368824\n",
      "\n",
      "21/03/22 22:49:48 INFO TransportClientFactory: Successfully created connection to /172.20.167.155:46291 after 17 ms (0 ms spent in bootstraps)\n",
      "\n",
      "21/03/22 22:49:48 INFO Utils: Fetching spark://172.20.167.155:46291/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.18.0.jar to /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/fetchFileTemp5287200041930828588.tmp\n",
      "\n",
      "21/03/22 22:49:48 INFO Executor: Adding file:/tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.18.0.jar to class loader\n",
      "\n",
      "21/03/22 22:49:48 INFO Executor: Fetching https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar with timestamp 1616453368823\n",
      "\n",
      "21/03/22 22:49:48 INFO Utils: Fetching https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar to /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/fetchFileTemp13430796167891695775.tmp\n",
      "\n",
      "21/03/22 22:49:49 INFO Executor: Adding file:/tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446/userFiles-a36a9aed-c599-4b5c-a9db-deeb73e63499/hadoop-aws-2.7.3.jar to class loader\n",
      "\n",
      "21/03/22 22:49:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver\n",
      "\n",
      "21/03/22 22:49:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 19085 ms on 172.20.167.155 (executor driver) (1/1)\n",
      "\n",
      "21/03/22 22:49:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "\n",
      "21/03/22 22:49:49 INFO DAGScheduler: ResultStage 0 (parquet at FileReader.scala:34) finished in 19.204 s\n",
      "\n",
      "21/03/22 22:49:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "\n",
      "21/03/22 22:49:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "\n",
      "21/03/22 22:49:49 INFO DAGScheduler: Job 0 finished: parquet at FileReader.scala:34, took 19.232560 s\n",
      "\n",
      "21/03/22 22:49:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.20.167.155:32879 in memory (size: 27.2 KiB, free: 434.4 MiB)\n",
      "\n",
      "21/03/22 22:49:50 INFO FileSourceStrategy: Pruning directories with: \n",
      "\n",
      "21/03/22 22:49:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(datetime),GreaterThanOrEqual(datetime,2020-10-10 00:00:00.0),LessThan(datetime,2020-10-20 00:00:00.0)\n",
      "\n",
      "21/03/22 22:49:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(datetime#8),(datetime#8 >= 1602288000000000),(datetime#8 < 1603152000000000)\n",
      "\n",
      "21/03/22 22:49:50 INFO FileSourceStrategy: Output Data Schema: struct<row_id: bigint, a: int, b: int, datetime: timestamp ... 2 more fields>\n",
      "\n",
      "21/03/22 22:49:50 INFO CodeGenerator: Code generated in 157.324 ms\n",
      "\n",
      "21/03/22 22:49:50 INFO CodeGenerator: Code generated in 20.6414 ms\n",
      "\n",
      "21/03/22 22:49:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 175.1 KiB, free 434.2 MiB)\n",
      "\n",
      "21/03/22 22:49:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.6 KiB, free 434.2 MiB)\n",
      "\n",
      "21/03/22 22:49:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.167.155:32879 (size: 25.6 KiB, free: 434.4 MiB)\n",
      "\n",
      "21/03/22 22:49:50 INFO SparkContext: Created broadcast 1 from rdd at RedisSinkRelation.scala:73\n",
      "\n",
      "21/03/22 22:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199788 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "\n",
      "21/03/22 22:49:51 INFO SparkContext: Starting job: foreachPartition at RedisSinkRelation.scala:79\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Got job 1 (foreachPartition at RedisSinkRelation.scala:79) with 1 output partitions\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Final stage: ResultStage 1 (foreachPartition at RedisSinkRelation.scala:79)\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Missing parents: List()\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at rdd at RedisSinkRelation.scala:73), which has no missing parents\n",
      "\n",
      "21/03/22 22:49:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 50.1 KiB, free 434.2 MiB)\n",
      "\n",
      "21/03/22 22:49:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 434.1 MiB)\n",
      "\n",
      "21/03/22 22:49:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.20.167.155:32879 (size: 18.8 KiB, free: 434.4 MiB)\n",
      "\n",
      "21/03/22 22:49:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at rdd at RedisSinkRelation.scala:73) (first 15 tasks are for partitions Vector(0))\n",
      "\n",
      "21/03/22 22:49:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "\n",
      "21/03/22 22:49:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 172.20.167.155, executor driver, partition 0, PROCESS_LOCAL, 7768 bytes)\n",
      "\n",
      "21/03/22 22:49:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "\n",
      "21/03/22 22:49:51 INFO FileScanRDD: Reading File path: file:///home/dvirdukhan/Code/feast/poc_table.parquet/1616453344.parquet, range: 0-5484, partition values: [empty row]\n",
      "\n",
      "21/03/22 22:49:51 INFO FilterCompat: Filtering using predicate: and(and(noteq(datetime, null), gteq(datetime, 1602288000000000)), lt(datetime, 1603152000000000))\n",
      "\n",
      "21/03/22 22:49:51 INFO FilterCompat: Filtering using predicate: and(and(noteq(datetime, null), gteq(datetime, 1602288000000000)), lt(datetime, 1603152000000000))\n",
      "\n",
      "21/03/22 22:49:51 INFO FilterCompat: Filtering using predicate: and(and(noteq(datetime, null), gteq(datetime, 1602288000000000)), lt(datetime, 1603152000000000))\n",
      "\n",
      "21/03/22 22:49:51 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "\n",
      "21/03/22 22:49:51 INFO MemoryStore: Block rdd_5_0 stored as values in memory (estimated size 528.0 B, free 434.1 MiB)\n",
      "\n",
      "21/03/22 22:49:51 INFO BlockManagerInfo: Added rdd_5_0 in memory on 172.20.167.155:32879 (size: 528.0 B, free: 434.4 MiB)\n",
      "\n",
      "21/03/22 22:49:51 INFO CodeGenerator: Code generated in 4.2804 ms\n",
      "\n",
      "21/03/22 22:49:51 INFO CodeGenerator: Code generated in 16.2327 ms\n",
      "\n",
      "21/03/22 22:49:51 INFO CodeGenerator: Code generated in 7.8607 ms\n",
      "\n",
      "21/03/22 22:49:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1809 bytes result sent to driver\n",
      "\n",
      "21/03/22 22:49:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 440 ms on 172.20.167.155 (executor driver) (1/1)\n",
      "\n",
      "21/03/22 22:49:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: ResultStage 1 (foreachPartition at RedisSinkRelation.scala:79) finished in 0.523 s\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "\n",
      "21/03/22 22:49:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "\n",
      "21/03/22 22:49:51 INFO DAGScheduler: Job 1 finished: foreachPartition at RedisSinkRelation.scala:79, took 0.529386 s\n",
      "\n",
      "21/03/22 22:49:51 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "\n",
      "21/03/22 22:49:51 INFO SparkUI: Stopped Spark web UI at http://172.20.167.155:48709\n",
      "\n",
      "21/03/22 22:49:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "\n",
      "21/03/22 22:49:51 INFO MemoryStore: MemoryStore cleared\n",
      "\n",
      "21/03/22 22:49:51 INFO BlockManager: BlockManager stopped\n",
      "\n",
      "21/03/22 22:49:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "\n",
      "21/03/22 22:49:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "\n",
      "21/03/22 22:49:51 INFO SparkContext: Successfully stopped SparkContext\n",
      "\n",
      "21/03/22 22:49:51 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "21/03/22 22:49:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-9495665c-3ded-4c79-911a-750f4567bd00\n",
      "\n",
      "21/03/22 22:49:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a54a881-6ab5-45da-a97a-c2145cfdd446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for stdout_line in iter(job._process.stderr.readline, \"\"):\n",
    "    print(stdout_line )\n",
    "    \n",
    "# for stdout_line in iter(job._process.stdout.readline, \"\"):\n",
    "#     print(stdout_line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neural-garden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SparkJobStatus.COMPLETED: 3>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiovascular-tulsa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RpcError",
     "evalue": "Unexpected error when pulling data from from Redis.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Code/feast/sdk/python/feast/client.py\u001b[0m in \u001b[0;36mget_model_run\u001b[0;34m(self, model_name, feature_refs, entity_rows, outputs, project)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             response = self._serving_service.OnlineModelRun(\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 OnlineModelRunRequest(\n",
      "\u001b[0;32m~/Code/feast/feast_env/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/feast/feast_env/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Unexpected error when pulling data from from Redis.\"\n\tdebug_error_string = \"{\"created\":\"@1616453576.969979100\",\"description\":\"Error received from peer ipv6:[::1]:6566\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1061,\"grpc_message\":\"Unexpected error when pulling data from from Redis.\",\"grpc_status\":2}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRpcError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a575c1d0277c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeast_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"poc_table:a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"poc_table:b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"row_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/feast/sdk/python/feast/client.py\u001b[0m in \u001b[0;36mget_model_run\u001b[0;34m(self, model_name, feature_refs, entity_rows, outputs, project)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             )\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOnlineResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRpcError\u001b[0m: Unexpected error when pulling data from from Redis."
     ]
    }
   ],
   "source": [
    "feast_client.get_model_run(\"graph\", [\"poc_table:a\", \"poc_table:b\"], [{\"row_id\": 1}], [\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interstate-fighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'row_id': [0, 1], 'poc_table:b': [2, 3]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = feast_client.get_online_features(\n",
    "    feature_refs=[\"poc_table:b\", \"poc_table:b\"],\n",
    "    entity_rows=[{'row_id':0}, {'row_id':1}]).to_dict()\n",
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feast_env",
   "language": "python",
   "name": "feast_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
